\section{Task 4: Neural Ranking Model}
RNN (Recurrent Neural Network) and LSTM (Long Short-Term Memory) are two types of neural network architectures used for various machine learning tasks including natural language processing.
\subsection{RNN}
The core idea behind RNN is to process sequential data by maintaining a hidden state that encodes information about previous inputs. The same set of weights is applied to each input in the sequence, allowing the network to handle variable-length input sequences. The memory comes from the hidden state of the network. The hidden state is updated at each time step of the input sequence and retains information about the previous inputs that the network has seen. The current input is combined with the previous hidden state using a set of weights, and the resulting output is used to update the hidden state for the next time step.
\\\\
One of the drawbacks of the RNN architecture is the vanishing gradient problem, where the gradients that are backpropagated through the network become increasingly small, making it difficult to learn long-term dependencies. Additionally, the RNN architecture can be computationally expensive due to the need to compute the hidden state for each input in the sequence.
\subsection{LSTM}
LSTM is a variant of RNN architecture designed to address the drawbacks of RNN. It introduces memory cells and gating mechanisms that allow the network to selectively remember or forget previous inputs based on the current input and past context. The memory cells are connected to input and output gates, which control the flow of information into and out of the cells. Additionally, there is a forget gate that can selectively erase information from the memory cells. However, LSTM's complex architecture may limit its use in real-time applications and make it more difficult to interpret and analyze.
\\\\
Considering the time-series related structure and relationships between query and passage, we choose LSTM as our neural ranking model because its ability to captures long term memory and avoids vanishing gradient.
\subsection{Data Process}
The terms of a query-passage pair is combined by add two terms together and only the first 100 terms will be used (after tokenization, the average number of terms in a query-passage pair is about 112). We choose model \textsl{glove-twitter-100} to represent each pair by a 100-dimension vector. For this pair longer than 100 terms, we only use the first 100 terms. For this pair shorter than 100 terms, we use zero vectors to fill the rest. Hence, for a query-passage pair, we have a 100-dimension vector to represent terms and each element in the vector is a 100-dimension vector to represent a term.