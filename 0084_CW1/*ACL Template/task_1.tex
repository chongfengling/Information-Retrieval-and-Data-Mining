\section{Task 1: Text Statistics}
\subsection{D1}
There are some text preprocessing method:
\begin{enumerate}
    \item Parsing:
    \begin{enumerate}
        \item Change the format of the file to raw text. No needed for the input file 'passage-collection.txt'.
        \item Identify structural elements: the contents in the input file are lines of sentences without titles, headings.
    \end{enumerate}
    \item Normalization:
    \begin{enumerate}
        \item Delete url from the document collection.
        \item All characters are converted to lowercase letters.
        \item Delete all non alpha characters.
    \end{enumerate}
    \item Tokenization
    \begin{enumerate}
        \item The input sentences are converted to tokens with the help of NLTK package.
    \end{enumerate}
    \item Lemmatization
    \begin{enumerate}
        \item Change a word to its base form.
    \end{enumerate}
    \item Stemming
    \begin{enumerate}
        \item Reduce inflected tokens to their root form.
    \end{enumerate}
\end{enumerate}
Here we only choose Normalization and Tokenization to preprocess documents and queries. If all stop words kept, our algorithm returns 127512 tokens, that is, vocabulary size is 127512. The top 10 tokens with the highest occurrences in the collection are [('the', 626848), ('of', 334267), ('a', 283665), ('and', 255205), ('to', 240957), ('is', 216854), ('in', 202195), ('for', 108149), ('or', 86925), ('you', 86659)]. Figure \ref{fig:1} and Figure \ref{fig:2} display the occurrence probability and Zipf's distribution with $k=1$ in different axis. The gap between empirical distribution and Zipf's distribution is small, especially when the rank is high. For the right end of the cuve, the Zipf's distribution is about 10 times larger than the empirical distribution, one reason could be that our tokens are too sophisticated. 
\\\\
Figure \ref{fig:3} and Figure \ref{fig:4} also show two distributions but the stop words are removed. In this case, the size of vocabulary is 127361 and the top 10 tokens with the highest occurrences are [('1', 43953), ('2', 33913), ('one', 27298), ('name', 25159), ('3', 22597), ('also', 21757), ('number', 21363), ('may', 20555), ('cost', 17127), ('used', 16540)]. According to the Zipf's law when $s=1$, we have 
\begin{equation}
    f(k ; s, N)=\frac{k^{-s}}{\sum_{i=1}^N i^{-s}} = \frac{1}{k \sum_{i=1}^N i^{-1}}
\end{equation}
where N is the vocabulary size and k is the terms's frequency rank. If we remove 
stop words, that is, some tokens with a high frequency and high rank are excluded and thus for the current token with a high rank, its frequency is reduced, cause a low occurrence probability in Figure \ref{fig:3} and \ref{fig:4}. For example, the term with rank 1, its probability drops by an order of magnitude from $10^{-1}$ to $10^{-2}$.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{../assets/Keep_stop_words.png}
    \caption{The occurrence probability against its occurrence rank for all terms and Zipf's distribution.}
    \label{fig:1}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{../assets/Keep_stop_words_(loglog).png}
    \caption{The occurrence probability against its occurrence rank for all terms and Zipf's distribution in log-log.}
    \label{fig:2}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{../assets/Remove_stop_words.png}
    \caption{The occurrence probability against its occurrence rank for all terms and Zipf's distribution.}
    \label{fig:3}
  \end{figure}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{../assets/Remove_stop_words_(loglog).png}
    \caption{The occurrence probability against its occurrence rank for all terms and Zipf's distribution in log-log.}
    \label{fig:4}
  \end{figure}
